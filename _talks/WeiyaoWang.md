---
title: "Adapting Visual Policies via Predicted Rewards"
collection: talks
type: "Graduate Research"
permalink: /talks/WeiyaoWang
venue: "Department of Computer Science, Johns Hopkins University"
date: 2023-09-20
location: "Baltimore, US"
---

My main contribution is to provide theoretical analysis on regret bounds under different pertubation.

Reinforcement learning with perturbed action: If the imitation learning algorithm learns to replicate the epxert's policy under the state distribution induced by the expert, this leads to a regret bound that grows quadratically in the time horizon of the task. [1]

Reinforcement learning with perturbed reward: Wait for write up...

Download published workshop paper [here](https://drive.google.com/file/d/10BI6IhTAY8Zf6pmeVfVk1GrxAssGb1g8/view)

[1] Ross, S., & Bagnell, D. (2010). Efficient Reductions for Imitation Learning. In Y. W. Teh & M. Titterington (Eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (pp. 661-668). PMLR. [PDF](https://www.google.com/search?client=safari&rls=en&q=Efficient+Reductions+for+Imitation+Learning&ie=UTF-8&oe=UTF-8)
